{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082d1e1d",
   "metadata": {},
   "source": [
    "# 03 â€” Regression\n",
    "\n",
    "When? Continuous value prediction (linear regression) or probability/class (logistic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reg = pd.read_csv(\"../data/linear_regression.csv\")\n",
    "\n",
    "# View data\n",
    "reg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e409eb",
   "metadata": {},
   "source": [
    "## Fitting a simple line with numpy.polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = np.polyfit(reg[\"x\"], reg[\"y\"], deg=1)\n",
    "slope, intercept = coef[0], coef[1]\n",
    "slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(reg[\"x\"], reg[\"y\"], alpha=0.6)\n",
    "xline = np.linspace(reg[\"x\"].min(), reg[\"x\"].max(), 100)\n",
    "yline = slope*xline + intercept\n",
    "plt.plot(xline, yline)\n",
    "plt.title(\"Linear Regression (polyfit)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0e000",
   "metadata": {},
   "source": [
    "Note: For more advanced regressions (categorical, diagnostic, logistic effects) you can use `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple logistics example with synthetic data\n",
    "from scipy.special import expit\n",
    "\n",
    "np.random.seed(1)\n",
    "X = np.random.normal(size=400)\n",
    "# Real model: logit(p)= -0.5 + 2*X\n",
    "p = expit(-0.5 + 2*X)\n",
    "y = np.random.binomial(1, p)\n",
    "\n",
    "# Very simple logistic gradient descent fitting (demonstration)\n",
    "# (For real project: statsmodels or scikit-learn is recommended.)\n",
    "beta0, beta1 = 0.0, 0.0\n",
    "lr = 0.01\n",
    "for _ in range(2000):\n",
    "    z = beta0 + beta1*X\n",
    "    pred = expit(z)\n",
    "    g0 = np.sum(pred - y)\n",
    "    g1 = np.sum((pred - y)*X)\n",
    "    beta0 -= lr*g0/len(X)\n",
    "    beta1 -= lr*g1/len(X)\n",
    "\n",
    "beta0, beta1"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
